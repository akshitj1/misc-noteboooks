{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/akshit.jain/repos/commoncrawl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer,StandardAnalyzer\n",
    "import os, os.path\n",
    "from whoosh import index\n",
    "from whoosh import writing\n",
    "from whoosh.writing import BufferedWriter, AsyncWriter\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh.query import Phrase, And,Term\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=pd.read_pickle('./data/indian_movies_cast.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar=pd.read_pickle('/Users/akshit.jain/repos/commoncrawl/output/parsed_pages.pk')\n",
    "ar=ar.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(body=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=True),\n",
    "                url=ID(stored=True), id=ID(stored=True))\n",
    "\n",
    "\n",
    "if not os.path.exists(\"indexdir\"):\n",
    "    os.mkdir(\"indexdir\")\n",
    "\n",
    "ix = index.create_in(\"indexdir\", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "#100\n",
      "#200\n",
      "#300\n",
      "#400\n",
      "#500\n",
      "#600\n",
      "#700\n",
      "#800\n",
      "#900\n",
      "#1000\n",
      "#1100\n",
      "#1200\n",
      "#1300\n",
      "#1400\n",
      "#1500\n",
      "#1600\n",
      "#1700\n",
      "#1800\n",
      "#1900\n",
      "#2000\n",
      "#2100\n",
      "#2200\n",
      "#2300\n",
      "#2400\n",
      "#2500\n",
      "#2600\n",
      "#2700\n",
      "#2800\n",
      "#2900\n",
      "#3000\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "This writer is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-7b6213c0094c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m#writer.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/whoosh/writing.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/whoosh/writing.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommitargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommitkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/whoosh/writing.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(self, mergetype, optimize, merge)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \"\"\"\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0;31m# Merge old segments if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mfinalsegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmergetype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/whoosh/writing.py\u001b[0m in \u001b[0;36m_check_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This writer is closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_doc_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexingError\u001b[0m: This writer is closed"
     ]
    }
   ],
   "source": [
    "with AsyncWriter(ix) as writer:\n",
    "    try:\n",
    "        writer.mergetype = writing.CLEAR\n",
    "        def add_doc(article, id):\n",
    "            writer.add_document(body=article.story, url=article.url, id=str(id))\n",
    "        for idx, article in ar.iteritems():\n",
    "            if idx%100==0:\n",
    "                print('#{}'.format(idx))\n",
    "            add_doc(article, idx)\n",
    "        writer.commit()\n",
    "    finally:\n",
    "        pass\n",
    "        #writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_occ=mc.groupby('actor').count().reset_index()\n",
    "popular_actors=actor_occ[actor_occ.movie>=10].sort_values('movie', ascending=False)\n",
    "popular_mc=mc[mc.actor.isin(popular_actors.actor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "#1000\n",
      "#2000\n",
      "#3000\n",
      "#4000\n",
      "#5000\n",
      "#6000\n",
      "#7000\n",
      "#8000\n",
      "#9000\n",
      "#10000\n",
      "#11000\n",
      "#12000\n",
      "#13000\n",
      "#14000\n",
      "#15000\n",
      "#16000\n",
      "#17000\n",
      "#18000\n",
      "#19000\n",
      "#20000\n",
      "#21000\n",
      "#22000\n",
      "#23000\n",
      "#24000\n",
      "#25000\n",
      "#26000\n",
      "#27000\n",
      "#28000\n",
      "#29000\n",
      "#30000\n",
      "#31000\n",
      "#32000\n",
      "#33000\n",
      "#34000\n",
      "#35000\n",
      "#36000\n",
      "#37000\n",
      "#38000\n",
      "#39000\n",
      "#40000\n",
      "#41000\n",
      "#42000\n",
      "#43000\n",
      "#44000\n",
      "#45000\n",
      "#46000\n",
      "#47000\n",
      "#48000\n",
      "#49000\n",
      "#50000\n",
      "#51000\n",
      "#52000\n",
      "#53000\n",
      "#54000\n",
      "#55000\n",
      "#56000\n",
      "#57000\n",
      "#58000\n",
      "#59000\n",
      "#60000\n",
      "#61000\n",
      "#62000\n",
      "#63000\n",
      "#64000\n",
      "#65000\n",
      "#66000\n",
      "#67000\n",
      "#68000\n",
      "#69000\n",
      "#70000\n",
      "#71000\n",
      "#72000\n",
      "#73000\n",
      "#74000\n",
      "#75000\n",
      "#76000\n",
      "#77000\n",
      "#78000\n",
      "#79000\n",
      "#80000\n",
      "#81000\n",
      "#82000\n",
      "#83000\n",
      "#84000\n",
      "#85000\n",
      "#86000\n",
      "#87000\n",
      "#88000\n",
      "#89000\n",
      "#90000\n",
      "#91000\n",
      "#92000\n",
      "#93000\n",
      "#94000\n",
      "#95000\n",
      "#96000\n",
      "#97000\n",
      "#98000\n",
      "#99000\n",
      "#100000\n",
      "#101000\n",
      "#102000\n",
      "#103000\n",
      "#104000\n",
      "#105000\n",
      "#106000\n",
      "#107000\n",
      "#108000\n",
      "#109000\n",
      "#110000\n",
      "#111000\n",
      "#112000\n",
      "#113000\n",
      "#114000\n",
      "#115000\n",
      "#116000\n",
      "#117000\n",
      "#118000\n",
      "#119000\n",
      "#120000\n",
      "#121000\n",
      "#122000\n",
      "#123000\n",
      "#124000\n",
      "#125000\n",
      "#126000\n",
      "#127000\n",
      "#128000\n",
      "#129000\n",
      "#130000\n",
      "#131000\n",
      "#132000\n",
      "#133000\n",
      "#134000\n",
      "#135000\n",
      "#136000\n",
      "#137000\n",
      "#138000\n",
      "#139000\n",
      "#140000\n",
      "#141000\n",
      "#142000\n",
      "#143000\n",
      "#144000\n",
      "#145000\n",
      "#146000\n",
      "#147000\n",
      "#148000\n",
      "#149000\n",
      "#150000\n",
      "#151000\n",
      "#152000\n",
      "#153000\n",
      "#154000\n",
      "#155000\n",
      "#156000\n",
      "#157000\n",
      "#158000\n",
      "#159000\n",
      "#160000\n",
      "#161000\n",
      "#162000\n",
      "#163000\n",
      "#164000\n",
      "#165000\n",
      "#166000\n",
      "#167000\n",
      "#168000\n",
      "#169000\n",
      "#170000\n",
      "#171000\n",
      "#172000\n",
      "#173000\n",
      "#174000\n",
      "#175000\n",
      "#176000\n",
      "#177000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "qp = QueryParser(\"body\", schema=ix.schema)\n",
    "s=ix.searcher()\n",
    "\n",
    "def get_matches(phrase, corpus):\n",
    "    sents = sent_tokenize(corpus)\n",
    "    for sent in sents:\n",
    "        matches = [m.start() for m in re.finditer(phrase.lower(), sent.lower())]\n",
    "        matches = [(start, start+len(phrase)) for start in matches]\n",
    "        if len(matches)>0:\n",
    "            yield {'sentence': sent, 'matches': matches}\n",
    "def sentence_matches_from_hits(phrase, hits):\n",
    "    for hit in hits:\n",
    "        yield from get_matches(phrase, hit['body']) \n",
    "\n",
    "\n",
    "def actor_movie_hits(row):\n",
    "    if row.name%1000==0:\n",
    "        print('#{}'.format(row.name))\n",
    "    q = qp.parse(u'\"{}\" AND \"{}\"'.format(row.actor, row.movie))\n",
    "    hits = s.search(q)\n",
    "    return list(sentence_matches_from_hits(row.movie, hits))\n",
    "_mc=popular_mc.reset_index()\n",
    "_mc['hits']=_mc.apply(actor_movie_hits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "    _mc['mentions']=_mc.apply(lambda row: list(sentence_matches_from_hits(row.movie, row.hits)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mc['num_hits']=_mc.hits.apply(len)\n",
    "_mc[_mc.num_hits>0].head()\n",
    "_mc=_mc[_mc.num_hits>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove movies made of only stop words\n",
    "from nltk.corpus import stopwords\n",
    "stoplist=frozenset(stopwords.words('english'))\n",
    "_mc=_mc[~_mc.movie.apply(lambda x: x.lower() in stoplist)]\n",
    "# _mc.sort_values('num_hits', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Anoushka has given the background score for the 1928 silent film, \"Shiraz: a Romance of India\", which was resurrected by BFI London.',\n",
       "  'matches': [(67, 73)]},\n",
       " {'sentence': \"'Shiraz' is an important part of our filmmaking and cultural history.\",\n",
       "  'matches': [(1, 7)]},\n",
       " {'sentence': 'But that affected the background score of \"Shiraz\" initially.',\n",
       "  'matches': [(43, 49)]},\n",
       " {'sentence': 'Post your comments  \\n\\n Anoushka Shankaranoushka shankar newsAnoushka Shankar Tourshiraz',\n",
       "  'matches': [(81, 87)]},\n",
       " {'sentence': '#Temptationtour #Toomuchfun #ThrowbackThursday #Madness #Happiness #Friendship #Ting😘\\u202c\" \\n  Credit: @Preity Zinta  \\n\\n \\r\\n The black and white photo sure brings back the fond memories from 2004 when Preity, Rani and Saif were every filmmaker\\'s favourite post the success of  Kal Ho Na Ho and Hum Tum, while Priyanka and Arjun were finding their mark in the industry.',\n",
       "  'matches': [(124, 139)]},\n",
       " {'sentence': '#GandhiJayanti— Madhuri Dixit Nene (@MadhuriDixit) October 2, 2018  \\rManoj Bajpayee: Ahimsa parmo dharm, Gandhi Jayanti ke shubh avsar par bapu ko shat shat naman aur sabhi desh vasiyo ko shubhkamnaye.',\n",
       "  'matches': [(98, 103)]},\n",
       " {'sentence': 'It is a story not without a sense of romance and just the sort of je na sai quoi so typical to India.',\n",
       "  'matches': [(37, 44)]},\n",
       " {'sentence': 'As we get to the climax and we get one of the most heartwarming moments of the film, it makes 96 a highly satisfying story of unfulfilled romance.',\n",
       "  'matches': [(138, 145)]},\n",
       " {'sentence': 'Sonam has featured in films of a variety of genres like romance, comedy, drama, biographical thriller and biographical comedy-drama among others.',\n",
       "  'matches': [(56, 63)]},\n",
       " {'sentence': \"In recent years, Anurag Kashyap's Phantom Films has produced movies like Mukkabaaz  (2017), and Masaan  (2015) that addressed the caste dynamics in our society and although these films received many accolades, they were not part of popular cinema.\",\n",
       "  'matches': [(152, 159)]}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mentions=[]\n",
    "def collect_mentions(x):\n",
    "    global all_mentions\n",
    "    all_mentions.extend(x)\n",
    "_mc.hits.apply(collect_mentions)\n",
    "all_mentions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions=pd.DataFrame(all_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_mentions(matches):\n",
    "    match_set=set()\n",
    "    for match_list in matches:\n",
    "        match_set = match_set | frozenset(match_list)\n",
    "    return list(match_set)\n",
    "movie_corpus=mentions.groupby('sentence').apply(lambda x:  union_mentions(x.matches)).to_frame('spans').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>spans</th>\n",
       "      <th>num_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>Actor Tiger Shroff grooved to hits like 'The P...</td>\n",
       "      <td>[(1750, 1756), (825, 836), (1463, 1481), (1319...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>\\nBest Director- Vikas Bahl for 'Queen'  Best ...</td>\n",
       "      <td>[(355, 367), (1494, 1506), (52, 57), (197, 203...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>Here is the Complete list of National Award 20...</td>\n",
       "      <td>[(2034, 2039), (2131, 2143), (2318, 2332), (23...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>He then starred in successful films such as Ji...</td>\n",
       "      <td>[(124, 133), (51, 58), (165, 172), (60, 66), (...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>The story is of Bitti who lives in Bareilly, b...</td>\n",
       "      <td>[(65, 66), (193, 194), (26, 27), (90, 91), (93...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "1318  Actor Tiger Shroff grooved to hits like 'The P...   \n",
       "62    \\nBest Director- Vikas Bahl for 'Queen'  Best ...   \n",
       "2593  Here is the Complete list of National Award 20...   \n",
       "2537  He then starred in successful films such as Ji...   \n",
       "4901  The story is of Bitti who lives in Bareilly, b...   \n",
       "\n",
       "                                                  spans  num_spans  \n",
       "1318  [(1750, 1756), (825, 836), (1463, 1481), (1319...         20  \n",
       "62    [(355, 367), (1494, 1506), (52, 57), (197, 203...         18  \n",
       "2593  [(2034, 2039), (2131, 2143), (2318, 2332), (23...         15  \n",
       "2537  [(124, 133), (51, 58), (165, 172), (60, 66), (...         11  \n",
       "4901  [(65, 66), (193, 194), (26, 27), (90, 91), (93...         11  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_corpus['num_spans']=movie_corpus['spans'].apply(len)\n",
    "movie_corpus.sort_values('num_spans', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_corpus.to_pickle('data/sentence_movie_occ_df.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_corpus=movie_corpus.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b class=\"match term0\">of</b> his shooting schedule for <b class=\"match term1\">Thugs</b> <b class=\"match term0\">of</b> <b class=\"match term2\">Hindostan</b>, which has been...But work on the '<b class=\"match term1\">Thugs</b> <b class=\"match term0\">of</b> <b class=\"match term2\">Hindostan</b>' reaches an end...was during the shoot <b class=\"match term0\">of</b> \"<b class=\"match term1\">Thugs</b> <b class=\"match term0\">of</b> <b class=\"match term2\">Hindostan</b>\" in Jodhpur when"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(hit.highlights('body',top=3,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re-train spacy ER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(x.sentence, {'entities': [(el[0], el[1], 'MOVIE') for el in x.spans]}) for idx,x in movie_corpus.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5770"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open('data/spacy_movie_ner_train.pk','wb') as f:\n",
    "    pk.dump(TRAIN_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_new_entity_type.py                      100% 4335     8.2KB/s   00:00    \n",
      "Killed by signal 1.\n"
     ]
    }
   ],
   "source": [
    "# ! scp data/spacy_movie_ner_train.pk gpu-sd:/mnt/data/akshit.jain/kg/data/\n",
    "! scp ./train_new_entity_type.py gpu-sd:/mnt/data/akshit.jain/kg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# new entity label\n",
    "LABEL = 'MOVIE'\n",
    "\n",
    "# training data\n",
    "# Note: If you're using an existing model, make sure to mix in examples of\n",
    "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
    "# model might learn the new type, but \"forget\" what it previously knew.\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "def main(model='en_core_web_md', new_model_name='movie_ner', output_dir='data/model', n_iter=10):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = 'Amir Khan gave a splendid performance in Dhoom 3'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For',\n",
       " 'a',\n",
       " 'story',\n",
       " 'that',\n",
       " 'unfolds',\n",
       " 'through',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'a',\n",
       " 'single',\n",
       " 'night',\n",
       " ',',\n",
       " '96',\n",
       " 'feels',\n",
       " 'long',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'for',\n",
       " 'its',\n",
       " 'own',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc, ents=TRAIN_DATA[0]\n",
    "import re\n",
    "def pre_process(doc):\n",
    "    al_pattern=r'([a-zA-Z]+)'\n",
    "    num_pattern=r'([\\d]+)'\n",
    "    doc=re.sub(al_pattern, r' \\1 ', doc)\n",
    "    doc=re.sub(num_pattern, r' \\1 ', doc)\n",
    "    doc=re.sub('\\s+',' ', doc)\n",
    "    return doc.split()\n",
    "pre_process(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Token():\n",
    "#     token=''\n",
    "#     tag=''\n",
    "\n",
    "def parse(doc, ents):\n",
    "    \n",
    "    ents=ents['entities']\n",
    "    entities=[]\n",
    "    for start,end, _ in ents:\n",
    "        entity=doc[start: end]\n",
    "        entity=re.sub('\\W',' ', entity)\n",
    "        entity = pre_process(entity)\n",
    "        entities.append(entity)\n",
    "    doc=pre_process(doc)\n",
    "    tags=['O']*len(doc)\n",
    "    for idx, tok in enumerate(doc):\n",
    "        for entity in entities:\n",
    "            if ' '.join([w.lower() for w in doc[idx: idx+len(entity)]])==' '.join([e.lower() for e in entity]):\n",
    "                # print('{} matched'.format(' '.join(entity)))\n",
    "                tags[idx: idx+len(entity)]=['M']*len(entity)\n",
    "    return doc, tags\n",
    "\n",
    "train_data_parsed=[]\n",
    "for doc, ents in TRAIN_DATA:\n",
    "    doc_tokens, tags=parse(doc, ents)\n",
    "    assert len(doc_tokens)==len(tags)\n",
    "    corpus=list(zip(doc_tokens, tags))\n",
    "    train_data_parsed.append(corpus)\n",
    "\n",
    "# import pickle as pk\n",
    "# with open('data/bert_ner_train_data.pk', 'wb') as f:\n",
    "#     pk.dump(train_data_parsed, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_SAMPLES=len(train_data_parsed)\n",
    "NUM_DEV=int(0.1*NUM_SAMPLES)\n",
    "NUM_TRAIN=NUM_SAMPLES-NUM_DEV\n",
    "\n",
    "train_data=train_data_parsed[0: NUM_TRAIN]\n",
    "dev_data=train_data_parsed[NUM_TRAIN:]\n",
    "\n",
    "import pickle as pk\n",
    "DATA_DUMP_DIR='/Users/akshit.jain/repos/BERT-NER/MovNERdata'\n",
    "with open(os.path.join(DATA_DUMP_DIR,'bert_ner_train_data.pk'), 'wb') as f:\n",
    "    pk.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(DATA_DUMP_DIR,'bert_ner_dev_data.pk'), 'wb') as f:\n",
    "    pk.dump(dev_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5770"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('For', 'O'), ('a', 'O'), ('story', 'O'), ('that', 'O'), ('unfolds', 'O'), ('through', 'O'), ('the', 'O'), ('course', 'O'), ('of', 'O'), ('a', 'O'), ('single', 'O'), ('night', 'O'), (',', 'O'), ('96', 'M'), ('feels', 'O'), ('long', 'O'), ('but', 'O'), ('it', 'O'), (\"'\", 'O'), ('s', 'O'), ('for', 'O'), ('its', 'O'), ('own', 'O'), ('good', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "for dat in train_data:\n",
    "    print(dat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/akshit.jain/repos/BERT-NER/output/predict_results.pk', 'rb') as f:\n",
    "    preds = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pataakha\n",
      "pataakha\n",
      "----------\n",
      "sanju\n",
      "sanju\n",
      "----------\n",
      "kabuliwala\n",
      "kabuliwala\n",
      "----------\n",
      "wedding\n",
      "wedding\n",
      "----------\n",
      "english vinglish\n",
      "english vinglish\n",
      "----------\n",
      "go housefull\n",
      "\n",
      "----------\n",
      "helicopter eela\n",
      "helicopter eela\n",
      "----------\n",
      "fanne khan\n",
      "fanne khan taal\n",
      "----------\n",
      "\n",
      "black\n",
      "----------\n",
      "dus ka dum\n",
      "dus ka dum\n",
      "----------\n",
      "good news\n",
      "good news\n",
      "----------\n",
      "manto\n",
      "manto mantonandita\n",
      "----------\n",
      "sultan bajrangi bhaijaan\n",
      "sultan bajrangi bhaijaan\n",
      "----------\n",
      "i am\n",
      "i am\n",
      "----------\n",
      "vip\n",
      "hero vip 2\n",
      "----------\n",
      "anbe sivam\n",
      "anbe sivam\n",
      "----------\n",
      "call\n",
      "\n",
      "----------\n",
      "sethupathi\n",
      "sir vijay sethupathi\n",
      "----------\n",
      "\n",
      "\n",
      "----------\n",
      "sunny\n",
      "sunny\n",
      "----------\n",
      "tumhari sulu kahaani hichki\n",
      "tumhari sulu kahaani hichki mardani\n",
      "----------\n",
      "\n",
      "\n",
      "----------\n",
      "plan\n",
      "plan\n",
      "----------\n",
      "nagavalli a aa\n",
      "a aa\n",
      "----------\n",
      "shani\n",
      "har har mahadev\n",
      "----------\n",
      "manto\n",
      "manto\n",
      "----------\n",
      "secret superstar\n",
      "secret superstar\n",
      "----------\n",
      "vip vip\n",
      "vip 2 vip 2\n",
      "----------\n",
      "nh 10\n",
      "nh 10\n",
      "----------\n",
      "raazi\n",
      "raazi\n",
      "----------\n",
      "\n",
      "\n",
      "----------\n",
      "queen\n",
      "queen\n",
      "----------\n",
      "\n",
      "\n",
      "----------\n",
      "company\n",
      "show drama\n",
      "----------\n",
      "bigg boss\n",
      "bigg boss\n",
      "----------\n",
      "\n",
      "\n",
      "----------\n",
      "judwaa 2\n",
      "judwaa 2\n",
      "----------\n",
      "\n",
      "\n",
      "----------\n",
      "lucky\n",
      "lucky\n",
      "----------\n",
      "bhoomi\n",
      "bhoomi\n",
      "----------\n",
      "helicopter eela\n",
      "helicopter eela\n",
      "----------\n",
      "gold\n",
      "gold\n",
      "----------\n",
      "helicopter eela\n",
      "helicopter eela october\n",
      "----------\n",
      "haider batti gul meter chalu\n",
      "haider batti gul meter chalu\n",
      "----------\n",
      "raavan\n",
      "raavan\n",
      "----------\n",
      "sunday\n",
      "\n",
      "----------\n",
      "manto\n",
      "manto\n",
      "----------\n",
      "sui dhaaga\n",
      "sui dhaaga\n",
      "----------\n",
      "culture\n",
      "culture\n",
      "----------\n",
      "golmaal again\n",
      "golmaal again\n",
      "----------\n",
      "indu sarkar\n",
      "indu sarkar\n",
      "----------\n",
      "o kadhal kanmani\n",
      "o kadhal kanmani\n",
      "----------\n",
      "chiranjeevi\n",
      "\n",
      "----------\n",
      "hu tu tu\n",
      "hu tu tu\n",
      "----------\n",
      "once upon a time in mumbaai the dirty picture\n",
      "\n",
      "----------\n",
      "\n",
      "ka aashiq\n",
      "----------\n",
      "partner housefull\n",
      "partner housefull don 2 david\n",
      "----------\n",
      "band baaja baaraat\n",
      "band baaja baaraat\n",
      "----------\n",
      "hindi medium\n",
      "hindi medium bed\n",
      "----------\n",
      "koffee with karan\n",
      "koffee with karan\n",
      "----------\n",
      "beautiful\n",
      "kashmir love\n",
      "----------\n",
      "102 not out\n",
      "102 not out\n",
      "----------\n",
      "ready\n",
      "bajrangi bhaijan\n",
      "----------\n",
      "kayamkulam kochunni\n",
      "kayamkulam kochunni\n",
      "----------\n",
      "pyaasa pyaasa\n",
      "pyaasa pyaasa friday\n",
      "----------\n",
      "aiyaary\n",
      "\n",
      "----------\n",
      "love\n",
      "aadukalam pink ghazi\n",
      "----------\n",
      "stree\n",
      "stree\n",
      "----------\n",
      "fukrey returns\n",
      "lamba fukrey returns\n",
      "----------\n",
      "justice\n",
      "justice\n",
      "----------\n",
      "roy\n",
      "\n",
      "----------\n",
      "dostana\n",
      "miami\n",
      "----------\n",
      "indian 2\n",
      "indian 2\n",
      "----------\n",
      "zero\n",
      "zero aur gangster 3\n",
      "----------\n",
      "star\n",
      "\n",
      "----------\n",
      "begum jaan\n",
      "begum jaan\n",
      "----------\n",
      "desire\n",
      "nene raju\n",
      "----------\n",
      "kaabil\n",
      "kaabil\n",
      "----------\n",
      "october\n",
      "october\n",
      "----------\n",
      "happy phirr bhag jayegi\n",
      "happy phirr bhag jayegi\n",
      "----------\n",
      "\n",
      "talkies 2\n",
      "----------\n",
      "saleem\n",
      "\n",
      "----------\n",
      "the challenge\n",
      "justice woman jaya\n",
      "----------\n",
      "lust stories\n",
      "lust stories\n",
      "----------\n",
      "dhoom fanaa\n",
      "bhooth dhoom fanaa fashion\n",
      "----------\n",
      "friendship\n",
      "friends friendship\n",
      "----------\n",
      "arjun\n",
      "leader\n",
      "----------\n",
      "the accidental prime minister vijay\n",
      "the accidental prime minister\n",
      "----------\n",
      "chennai express\n",
      "chennai express\n",
      "----------\n",
      "drive\n",
      "drive\n",
      "----------\n",
      "102 not out 102 not out\n",
      "102 not 102 not out\n",
      "----------\n",
      "kabhie kabhie trishul deewar\n",
      "kabhie kabhie deewar\n",
      "----------\n",
      "i am\n",
      "i am english\n",
      "----------\n",
      "udta punjab\n",
      "dangal udta punjab\n",
      "----------\n",
      "sui dhaaga\n",
      "sui dhaaga\n",
      "----------\n",
      "gully boy\n",
      "gully boy\n",
      "----------\n",
      "manto\n",
      "manto\n",
      "----------\n",
      "naam\n",
      "naam\n",
      "----------\n",
      "thank you\n",
      "thank you\n",
      "----------\n",
      "two\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def extract_entity(tokens, labels):\n",
    "    movie_mask=np.array([False]*len(labels))\n",
    "    for idx, lbl in enumerate(labels):\n",
    "        if labels[idx]==2 or (movie_mask[idx-1] and (tokens[idx][:2]=='##')):\n",
    "            movie_mask[idx]=True    \n",
    "    movie_mask=movie_mask[:len(tokens)]\n",
    "    return re.sub(r' ##', '',' '.join(list(np.array(tokens)[movie_mask])))\n",
    "for p in preds[100:200]:\n",
    "    print(extract_entity(p['tokens'], p['labels_true']))\n",
    "    print(extract_entity(p['tokens'], p['labels_pred']))\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res=[[int(x) for x in line.split()] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_single_example(20, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list([1,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "pydefault"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
